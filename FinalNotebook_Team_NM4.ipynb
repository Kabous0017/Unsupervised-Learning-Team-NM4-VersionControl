{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c7e849a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T09:24:53.643384Z",
     "start_time": "2021-06-11T09:24:53.622385Z"
    }
   },
   "source": [
    "# Unsupervised Learning Predict Student Solution\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "### Honour Code\n",
    "\n",
    "I {**Kobus Le Roux, Devon Woodman, Nhlanhla Mthembu, Koketso Maraba, Tebogo Khoza, Mxolisi Zulu, Cara Brits**}, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "### Predict Overview: Movie Recommendation\n",
    "\n",
    "__UPDATE ME__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9c90b2b",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "__UPDATE ME__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05600c92",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Conclusions</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "997462e2",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42960f2f",
   "metadata": {},
   "source": [
    "Let's start by turning off unnecessary warnings in the code, to improve the aesthetics of our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "65a0209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning off unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4f12ae4",
   "metadata": {},
   "source": [
    "First, we include a list of uncommon packages that may need to be installed on your system. Uncomment and run the following if you need some of the packages listed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "0badc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wordcloud\n",
    "# !pip install contractions\n",
    "# !pip install catboost\n",
    "# !pip install xgboost\n",
    "# !pip install comet_ml\n",
    "# !pip install emoji\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14432976",
   "metadata": {},
   "source": [
    "Next, we import all necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "c5324d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kobus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Libraries for Data Loading and Manipulation\n",
    "import pandas as pd  # Pandas provides flexible data structures to manipulate structured data.\n",
    "import numpy as np  # Numpy offers powerful data structures and tools for numerical computation.\n",
    "\n",
    "# Text Processing Libraries\n",
    "import contractions  # Contractions is used to handle English contractions, converting them into their longer forms.\n",
    "import emoji  # Emoji allows easy manipulation and analysis of emojis in the text.\n",
    "from nltk.corpus import stopwords  # Stopwords module provides a list of common words to be removed from the text.\n",
    "from nltk.stem import WordNetLemmatizer  # WordNetLemmatizer is used for lemmatizing words, bringing them to their root form.\n",
    "from nltk import download as nltk_download  # For downloading nltk packages, here 'wordnet'.\n",
    "import regex  # Regex is used for regular expression matching and manipulation.\n",
    "import string  # Provides constants and classes for string manipulation.\n",
    "import re # Provides common string operations\n",
    "import unicodedata  # Provides access to the Unicode Character Database for processing Unicode characters.\n",
    "\n",
    "#Data monitoring Libraries:\n",
    "from comet_ml import Experiment  # Allows developers to track, compare, explain and optimize experiments and models.\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from catboost import CatBoostClassifier  # High-performance gradient boosting on decision trees library.\n",
    "from scipy.sparse import hstack  # Used for stacking sparse matrices horizontally.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer  # Transforms text into feature vectors for machine learning.\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, confusion_matrix  # Metrics for evaluating machine learning models.\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV  # Provides utilities to split data and tune hyperparameters.\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB  # Naive Bayes classifiers for machine learning.\n",
    "from sklearn.linear_model import LogisticRegression # Logistic Regression classifier for machine learning.\n",
    "from sklearn.tree import DecisionTreeClassifier # Decision Tree classifier for machine learning.\n",
    "from sklearn.ensemble import RandomForestClassifier # Random Forrest classifier for machine learning\n",
    "from sklearn.svm import LinearSVC, SVC  # Support Vector Machine classifiers.\n",
    "from xgboost import XGBClassifier  # XGBoost is an efficient and flexible gradient boosting library.\n",
    "import tensorflow as tf # For use in Neural Net\n",
    "from tensorflow.keras.layers import Dense # For building neural net \n",
    "from tensorflow.keras.models import Sequential # For building neural net \n",
    "from tensorflow.keras import utils, backend as K # For neural net add ons \n",
    "\n",
    "# Feature selection Libraries:\n",
    "from sklearn.feature_selection import SelectKBest # To reduce features\n",
    "from sklearn.feature_selection import chi2 # Used to estimate which features is most impactful\n",
    "\n",
    "# Data Visualization Libraries\n",
    "import matplotlib.pyplot as plt  # Provides a way to visualize data (plotting, charts, etc.).\n",
    "import seaborn as sns  # Provides a high-level interface for attractive, informative statistical graphics.\n",
    "from sklearn.metrics import ConfusionMatrixDisplay # To visualize confusion matirces\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator  # Wordcloud is used to visualize word frequency in the text.\n",
    "sns.set_theme(style=\"whitegrid\")  # Sets the style for seaborn plots.\n",
    "\n",
    "# Utility Libraries\n",
    "import pickle # For saving models to file\n",
    "from sklearn.utils import resample # For resampling unbalanced data\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "\n",
    "# Downloading necessary NLTK corpus\n",
    "nltk_download('wordnet')\n",
    "\n",
    "# Global Constants for reproducibility and consistency\n",
    "TRAIN_TEST_SPLIT_VAR = 0.2\n",
    "RAND_STATE = 42\n",
    "MAX_TEXT_FEATURES = 50000 # For all algorithms with exception to those listed below\n",
    "RAD_SVC_Text_Features = 3000 # Specifically for use in radial svc\n",
    "NN_Text_Features = 5000 #Specifically for use of neural net\n",
    "XGB_Text_Features = 3000 # Specifically for use in XGBoost algorithm\n",
    "RESAMPLE_ENTRY_AMOUNT = 4500 # If resampling, how many entries per category (Between 1000 and 6000)\n",
    "VEC_MIN_WORD_TO_REMOVE = 1 # Remove words that occurs less that this value in dataset \n",
    "\n",
    "\n",
    "# Flags for notebook Execution\n",
    "COMET_FLAG = False # To gauge wether to commit experiments to Comet ML\n",
    "VECTORIZER_TO_USE = \"tfidf\" # Chooses between TfIDF vectorizer or count Vectorizer - accepted values are \"tfidf\" or \"count\"\n",
    "RESAMPLE = False # Whether to resample training data\n",
    "REMOVE_OVERUSED_WORDS = False # Wether to remove non-class specific words that occurs a lot\n",
    "\n",
    "# Comet variables for logging experiments\n",
    "COMET_API_KEY = \"###########\"\n",
    "COMET_PROJECT_NAME = \"########\"\n",
    "COMET_WORKSPACE = \"#########\"\n",
    "\n",
    "\n",
    "# Select which models to tune hyperparameters on\n",
    "HYPERPARAM_TO_Tune = {\n",
    "    \"Logistic Regression\" : False,\n",
    "    \"Decision Tree\": False,\n",
    "    \"Random Forrest\": False,\n",
    "    \"Linear Support Vector\": False,\n",
    "    \"Radial Support Vector\": False,\n",
    "    \"Gaussian Naive Bayes\": False,\n",
    "    \"Multinomial Naive Bayes\": False,\n",
    "    \"CatBoost\": False,\n",
    "    \"XGBoost\": False\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f22a6718",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "We load the data from the datafiles:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81132ab3",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "Before we can jump in and construct models for the data, it is important to investigate and understand the features present within the dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fa93ec6",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data Engineering and NLP Preprocessing\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf942ceb",
   "metadata": {},
   "source": [
    "To capture all the preprocessing happening within the data, we start a comet experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "131a6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMET_FLAG:\n",
    "  # Start comet experiment:\n",
    "  experiment = Experiment(\n",
    "    api_key = COMET_API_KEY,\n",
    "    project_name = COMET_PROJECT_NAME,\n",
    "    workspace= COMET_WORKSPACE\n",
    "  )\n",
    "\n",
    "  experiment.add_tag(\"Data Preprocessing\")\n",
    "  experiment.set_name(\"Data Preprocessing\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ba1cdeb",
   "metadata": {},
   "source": [
    "We first begin by reloading the data and discarding the changes made during the EDA section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df7e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adfd77a0",
   "metadata": {},
   "source": [
    "Ending the comet experiment for data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "ae064718",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMET_FLAG:\n",
    "    experiment.end()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43b2d523",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b1d7734",
   "metadata": {},
   "source": [
    "### 5.1 Logistic regression\n",
    "\n",
    "Logistic regression is a classification algorithm used to predict the probability of a binary outcome based on one or more input features. It models the relationship between the input variables and the probability of the outcome belonging to a particular class. Logistic regression uses the logistic function (also known as the sigmoid function) to map the output of a linear combination of the input features to a value between 0 and 1, representing the probability of belonging to the positive class.\n",
    "In simpler terms, logistic regression aims to find the best-fitting S-shaped curve that separates the two classes. It estimates the coefficients (weights) of the input features through a process called maximum likelihood estimation, optimizing the parameters to maximize the likelihood of the observed data.\n",
    "\n",
    "Once trained, logistic regression can make predictions by calculating the probability of the positive class based on the input features. A threshold is then applied to determine the final predicted class.\n",
    "\n",
    "Logistic regression models are known for their simplicity and interpetability. Since they are more simplistic models, they are relatively quick to train and computationally efficient. They can also be expanded to handle multiclass classification as is the case for our data. This model does assume a linear relationship between the features and the log-odds of the outcome, however, which does not necessarily hold true in many cases. It is also sensitive to outliers and irrelevant features. \n",
    "\n",
    "Lets begin our training process, by starting our Comet ML experiment, so that all aspects of the model gets logged for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "5301b781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment not logged\n"
     ]
    }
   ],
   "source": [
    "if COMET_FLAG:\n",
    "  # Start comet experiment:\n",
    "  experiment = Experiment(\n",
    "    api_key = COMET_API_KEY,\n",
    "    project_name = COMET_PROJECT_NAME,\n",
    "    workspace= COMET_WORKSPACE\n",
    "  )\n",
    "\n",
    "  experiment.add_tag(\"LogisticRegression\")\n",
    "  experiment.set_name(\"LogisticRegression\")\n",
    "\n",
    "else:\n",
    "  print(\"Experiment not logged\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "498161b4",
   "metadata": {},
   "source": [
    "We begin by initializing a Logistic Regression classifier , and fitting it to the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "8d3924c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-17 {color: black;background-color: white;}#sk-container-id-17 pre{padding: 0;}#sk-container-id-17 div.sk-toggleable {background-color: white;}#sk-container-id-17 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-17 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-17 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-17 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-17 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-17 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-17 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-17 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-17 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-17 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-17 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-17 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-17 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-17 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-17 div.sk-item {position: relative;z-index: 1;}#sk-container-id-17 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-17 div.sk-item::before, #sk-container-id-17 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-17 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-17 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-17 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-17 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-17 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-17 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-17 div.sk-label-container {text-align: center;}#sk-container-id-17 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-17 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-17\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" checked><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a Logistic Regression Classifier object:\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Fitting the model to our training data subset\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5b1cba4",
   "metadata": {},
   "source": [
    "To get a sense of how well this model is performing, let's test the model on the testing subset of our training data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "a26ad108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.82      0.26      0.39       278\n",
      "         0.0       0.62      0.38      0.47       425\n",
      "         1.0       0.73      0.90      0.81      1755\n",
      "         2.0       0.78      0.74      0.76       706\n",
      "\n",
      "    accuracy                           0.73      3164\n",
      "   macro avg       0.74      0.57      0.61      3164\n",
      "weighted avg       0.73      0.73      0.71      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict values of the testing subset\n",
    "pred_log_reg = log_reg.predict(X_test)\n",
    "\n",
    "# Let's produce a classification report of the model as is:\n",
    "print(classification_report(y_test, pred_log_reg))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de501dda",
   "metadata": {},
   "source": [
    "Form the classification report, we see that our recall is relatively low for our two least represented categories, namely `Anti climate change` and `Neutral` sentiments. Our remaining sentiments scores pretty high in all the metrics we observe. This model performs pretty well in overall predictive performance.\n",
    "\n",
    "\n",
    "If interested, one can try to gauge the effects of hyperparameter tuning on the model performance. Note: For the purposes of this notebook, hyperparameter tuning will be ignored since it can be very timely to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "6884db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HYPERPARAM_TO_Tune[\"Logistic Regression\"]:\n",
    "    # Set hyper-parameters to tune \n",
    "    log_reg_hyperparameters =     {\n",
    "    'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C' : np.logspace(-4, 4, 5),\n",
    "    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n",
    "    'max_iter' : [100, 1000,2500]\n",
    "    } \n",
    "\n",
    "    # Search over hyperparameter grid\n",
    "    log_reg_gridsearch = GridSearchCV(LogisticRegression(), log_reg_hyperparameters, scoring='f1_weighted', cv =3, n_jobs=3, verbose=3) \n",
    "\n",
    "    # Fit grid of models to the data\n",
    "    log_reg_gridsearch.fit(X_train, y_train)\n",
    "    best_params = log_reg_gridsearch.best_params_\n",
    "else:\n",
    "    log_reg_hyperparameters = {\n",
    "    'penalty' : \"default\",\n",
    "    'C' : \"default\",\n",
    "    'solver' : \"default\",\n",
    "    'max_iter' : \"default\"\n",
    "    } \n",
    "\n",
    "    best_params = {\n",
    "    'penalty' : 'l2',\n",
    "    'C' : 1.0,\n",
    "    'solver' : 'lbfgs',\n",
    "    'max_iter' : 100\n",
    "    } "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3a8362e",
   "metadata": {},
   "source": [
    "Let us save our metrics for our model considered, in order to log it to CometML later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "bd359f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HYPERPARAM_TO_Tune[\"Logistic Regression\"]:\n",
    "    # Get best predictor:\n",
    "    best_log_reg = log_reg_gridsearch.best_estimator_\n",
    "\n",
    "    # Use best estimator to make predictions\n",
    "    pred_best_log_reg = best_log_reg.predict(X_test)\n",
    "\n",
    "    # Print optimised classification report:\n",
    "    print(classification_report(y_test, pred_best_log_reg))\n",
    "\n",
    "    # Produce confusion matrix, f1 score, precision and recall for model in order to log them to comet\n",
    "    log_reg_cm = confusion_matrix(y_test, pred_best_log_reg)\n",
    "    log_reg_f1 = f1_score(y_test, pred_best_log_reg, average=\"weighted\") # average only necessary if model nly deals in binary classification\n",
    "    log_reg_precision = precision_score(y_test, pred_best_log_reg, average='weighted')\n",
    "    log_reg_recall = recall_score(y_test, pred_best_log_reg, average=\"weighted\")\n",
    "\n",
    "else:\n",
    "    log_reg_cm = confusion_matrix(y_test, pred_log_reg)\n",
    "    log_reg_f1 = f1_score(y_test, pred_log_reg, average=\"weighted\") # average only necessary if model nly deals in binary classification\n",
    "    log_reg_precision = precision_score(y_test, pred_log_reg, average='weighted')\n",
    "    log_reg_recall = recall_score(y_test, pred_log_reg, average=\"weighted\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4eaa0184",
   "metadata": {},
   "source": [
    "\n",
    "Next, we log the metrics calculated above to use within comet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "3b35dbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logging info\n",
    "\n",
    "# Add all relevant info pertaining to your model here\n",
    "log_reg_params = {\"random_state\": RAND_STATE, # DO not remove this one\n",
    "          \"model_type\": \"log_reg\",\n",
    "          \"scaler\": \"no scaling\", # DO not remove this one\n",
    "          \"param_grid\": str(log_reg_hyperparameters),\n",
    "          \"best_param\": str(best_params),\n",
    "          \"text_feature_count\": str(MAX_TEXT_FEATURES), # DO not remove this one\n",
    "          \"vectorizer\": VECTORIZER_TO_USE, # DO not remove this one\n",
    "          \"train_test_split\": str(TRAIN_TEST_SPLIT_VAR), # DO not remove this one\n",
    "          \"Resample\": str(RESAMPLE)\n",
    "          }\n",
    "log_reg_metrics = {\"f1\": log_reg_f1,\n",
    "           \"recall\": log_reg_recall,\n",
    "           \"precision\": log_reg_precision,\n",
    "           \"confusion_matrix\": log_reg_cm\n",
    "           }\n",
    "\n",
    "if COMET_FLAG:\n",
    "    # Log all important info\n",
    "    experiment.log_parameters(log_reg_params)\n",
    "    experiment.log_metrics(log_reg_metrics)\n",
    "    experiment.log_confusion_matrix(matrix=log_reg_cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad9ca5df",
   "metadata": {},
   "source": [
    "In order to produce a submission to Kaggle, we retrain the model (using the optimised hyperparameters if applicable) on the complete set of training data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "e431a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate final model\n",
    "final_log_reg = LogisticRegression( **best_params) # Use determined hyperparameters here\n",
    "\n",
    "# Train on all available data\n",
    "final_log_reg.fit(x_train_dataset, y_train_dataset)\n",
    "\n",
    "# Generate predictions for the evaluation dataset\n",
    "log_reg_kaggle_predictions = final_log_reg.predict(x_evaluate_dataset)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00ac3a85",
   "metadata": {},
   "source": [
    "Finally, we produce a csv of the predictions to submit to Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "7cfc1c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>169760</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>224985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>476263</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>872928</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10541</th>\n",
       "      <td>895714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10542</th>\n",
       "      <td>875167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10543</th>\n",
       "      <td>78329</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10544</th>\n",
       "      <td>867455</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10545</th>\n",
       "      <td>470892</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10546 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweetid  sentiment\n",
       "0       169760          1\n",
       "1        35326          1\n",
       "2       224985          1\n",
       "3       476263          1\n",
       "4       872928          1\n",
       "...        ...        ...\n",
       "10541   895714          1\n",
       "10542   875167          1\n",
       "10543    78329          2\n",
       "10544   867455          0\n",
       "10545   470892          1\n",
       "\n",
       "[10546 rows x 2 columns]"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Produce kaggle submission csv \n",
    "\n",
    "# Make submissions into dataframe\n",
    "df_log_reg_kaggle_predictions = pd.DataFrame(log_reg_kaggle_predictions, columns=[\"sentiment\"])\n",
    "\n",
    "# Convert sentiment to integer\n",
    "df_log_reg_kaggle_predictions['sentiment'] = df_log_reg_kaggle_predictions['sentiment'].astype('int')\n",
    "\n",
    "# Match submissions to tweet id's\n",
    "df_log_reg_kaggle_submission = pd.concat([df_tweetid_for_submission.reset_index(drop=True), df_log_reg_kaggle_predictions.reset_index(drop=True)], axis=1,)\n",
    "\n",
    "\n",
    "#Save submission as csv\n",
    "df_log_reg_kaggle_submission.to_csv('LogisticRegressionPredictions.csv', index=False)\n",
    "\n",
    "df_log_reg_kaggle_submission"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2ae5d4b",
   "metadata": {},
   "source": [
    "Lastly, since we are done with the model, we remember to end the experiment and write logs to Comet ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "8f38b29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Logistic Regression: Experiment not logged\n"
     ]
    }
   ],
   "source": [
    "if COMET_FLAG:\n",
    "    # Ending the experiment\n",
    "    experiment.end()\n",
    "else:\n",
    "    print(\"Completed Logistic Regression: Experiment not logged\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "401a6f5d",
   "metadata": {},
   "source": [
    "Saving the model for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "14ed3131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a filename to save model as\n",
    "model_save_path = './Models/LogisticRegression.pkl'\n",
    "\n",
    "# Save model to specified filename\n",
    "with open(model_save_path, 'wb') as file:\n",
    "    pickle.dump(final_log_reg,file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cf71ad4",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b530251",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 6. Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "To evaluate our model performance, we will make use of the `f1-score` as a metric...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3e993ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e955e527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5978e5a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e1ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8ad0c0d",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Conclusion\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4904da0",
   "metadata": {},
   "source": [
    "In conclusion, our best performing model for predicting sentiment...\n",
    "\n",
    "#### Future Considerations:\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "663c1fa3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
